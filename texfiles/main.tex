\documentclass[12pt]{article}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[backend=biber, style=apa]{biblatex}

\usepackage{mathtools}

\usepackage{acronym}

\usepackage[nameinlink,noabbrev]{cleveref} % nice for referencing


\crefname{equation}{equation}{equations}
\Crefname{equation}{Equation}{Equations}
\crefname{table}{table}{tables}
\Crefname{table}{Table}{Tables}
\crefname{figure}{fig.}{figures}
\Crefname{figure}{Fig.}{Figures}

\title{Tittel}
\author{Forfatter}
\date{}


\begin{document}

\maketitle

\section{Introduction}


\section{Background}


\section{Method}
The purpose of this project, as mentioned earlier, is to understand the math and implementations underlying a recurrent neural network (RNN). For the implementation of the RNN, no traditional machine learning (ML) libraries has been utilised. The RNN model and its surroundings are implemented using Python, where numpy \cite{NUMPY} is the library taking care of the mathematical implementations such as vector and matrix multiplication. The RNN is implemented in its most basic form, often denoted as a \textit{vanilla RNN}. Nevertheless, the implementation handles a big variety of data types, and allows for configurability of optimisation algorithms and activation functions. The focus of the implementation has been natural language data, but the model can also be used for other time series data, such as learning to predict interest rates, or mimic and generate oscillations like sine waves. None of the implementation exploits graphical processing units (GPUs), and is therefore slower than more advanced implmentations where the power of GPUs is taken advantage of.


\subsection{Python implementation}

\subsubsection*{Model architecture}
The RNN is built as a class, where all nodes and layers are built simply by initialising matrices of dimensions corresponding to the layers. Optimiser, loss function and activation function(s) are initialised at the time of model initialisation. Training and inference is done using these functions. The parameters of the model are always 5: Weights from input to hidden layer (also called the recurrent layer), denoted \texttt{w\_xh}; weights from hidden to hidden, denoted \texttt{w\_hh}; and weights from hidden to output layer, denoted \texttt{w\_hy}. The hidden and output layer also have bias parameters, \texttt{b\_hh} and \texttt{b\_hy}, respectively. The weights and biases are the only parameters learned through experience. In addition, hyperparameters such as number of fully connected hidden nodes, learning rate and number of backpropagation steps are controlled by the user \color{red}(is activation function etc also hyperparameters?)\color{black}. 

\subsubsection*{Random control}
All experiments using the model is seeded, to ensure reproducable results. Initialisation values of all parameters (not hyperparameters) are drawn from the unit normal distribution \color{red} check this \color{black}, to a value between 0 and 1, multiplied by a scale specified by the user.



\section{Results}

\section*{Appendix - some math notes}

\subsection*{Backpropagation through time - BPTT}
Definitions:
\begin{align}
    \mathbf{a}^{(t)} &= \mathbf{b + Wh^{(t-1)} + Ux^{(t)}} \\
    \mathbf{h}^{(t)} &= activation(\mathbf{a}^{(t)})\\
    \mathbf{o}^{(t)} &= \mathbf{c + Vh}^{(t)} \\
    \mathbf{\hat{y}}^{(t)} &= softmax(\mathbf{o}^{(t)}) \\
    \mathbf{U} &= \text{input weights} \\
    \mathbf{W} &= \text{hidden weights} \\
    \mathbf{V} &= \text{output weights}
\end{align}


The goal is to maximize the probability of the observed data by estimating parameters (here: $\mathbf{U, V, W, b, c}$). The estimated parameters yielding the highest maximum likelihood are called the maximum likelihood estimates. These parameters can be estimated by minimizing the cross-entropy between the model distribution and the data. %To ease calculations, and because of the concavity of the problem, we derive the following cost-function:
This cross-entropy function, which is a function of inputs ($\mathbf{x}$) and outputs ($\mathbf{y}$), can be seen in (8).
\begin{align}
    &C\left(\{\mathbf{x}^{(1)},...,\mathbf{x}^{(\tau)}\}, \{\mathbf{y}^{(1)},...,\mathbf{y}^{(\tau)}\}\right) \\
    &= \sum_t C^{(t)} \\
    &= -\sum_t log\;p\left(y^{(t)}|\{\mathbf{x}^{(1)},...,\mathbf{x}^{(t)}\}\right) \\
\end{align}

Note: $\quad y^{(t)} \;\;$ is an entry in the output vector $\;\; \mathbf{\hat{y}}^{(t)}$ \par
The cost function is the negative log-likelihood function. Minimising this function is the same as maximum the likelihood of the parameters - not due to the log, but due to the negative prefix. The log in log-likelihood works, but I dont know why it is used? \par
Below we derive the gradients of the nodes in the computational graph from the deep learning book. These gradients must propagate backwards through time, from time $t=\tau$ down to $t=0$.
\par
The gradient of the cost function at the output, $\mathbf{o}$, at time \textit{t} is
\begin{align}
    \nabla_{\mathbf{o}^{(t)}} C &= \frac{\delta C}{\delta o^{(t)}}
    = \frac{\delta C}{\delta C^{(t)}}\frac{\delta C^{(t)}}{\delta o^{(t)}}
    = \hat{y}^{(t)} - \mathbf{1}_{i=y^{(t)}}
    % \text{i is indices in $\hat{y}^{(t)}$. The predicate at the end of the previous line makes sence if one considers for example the $\hat{y}^{(t)}$ consisting of probabilities for all the different characters.}
\end{align}

We have found a general expression for the gradien at the $\mathbf{o}^{(t)}$-nodes. The next step is to find an expression for the gradient of the final hidden (computational) node, at time $\tau$: $\mathbf{h}^{(\tau)}$. Its only descendant is $\mathbf{o}^{(\tau)}$, which means its gradient is solely dependent on this node, which makes it a good starting point for the later gradient calculations.

\begin{align}
    \nabla_{\mathbf{h}^{(\tau)}} C &= \left(\nabla_{\mathbf{o}^{(\tau)}}C\right) \frac{\delta \mathbf{o}^{(\tau)}}{\delta \mathbf{h}^{(\tau)}}\\
    &= \left(\nabla_{\mathbf{o}^{(\tau)}}C\right) \mathbf{V}\\
    \nabla_{\mathbf{h}^{(\tau)}} C &= \mathbf{V}^{\top} \left(\nabla_{\mathbf{o}^{(\tau)}}C\right)
    % \nabla_{\mathbf{h}{(\tau)}} C &= \mathbf{V}^{\top}\left(\hat{y}^{(\tau)} - \mathbf{1}_{i=y^{(\tau)}}\right) \\
\end{align}
Where all the right hand side terms are known from before. \par

The only nodes that need gradient computation now, are all the hidden states preceding the last. I.e., for $\mathbf{h}^{(t)}$, where $t = \{0,...,\tau-1\}$. For these time steps, the gradient is influenced by both the gradient at $\mathbf{o}^{(t)}$, as well as all the preceding hidden state gradients. Remember that the preceding hidden state of $\mathbf{h}^{(t)}$ is $\mathbf{h}^{(t+1)}$, which has preceding hidden state $\mathbf{h}^{(t+2)}$, and so on. We are calculating the gradient starting from $t=\tau-1$, working our way down to $t=0$: \par
% \begin{align}
%     \shortintertext{Denne linja kan v√¶re veldig feil}
%     \nabla_{\mathbf{h}^{(t-1)}} C &= \nabla_{\mathbf{h}^{(t)}} C + \nabla_{\mathbf{o}^{(t-1)}}C \\ 
%     \nabla_{\mathbf{h}^{(t-1)}} C &= \nabla_{\mathbf{h}^{(t)}} C \frac{\delta}{\delta} + V^{\top}\left(\nabla_{\mathbf{o}^{(t-1)}}C\right)
% \end{align}

\begin{align}
    \nabla_{\mathbf{h}^{(\tau-1)}} C &= \color{blue}\nabla_{\mathbf{o}^{(\tau-1)}}C \frac{\delta \mathbf{o}^{(\tau-1)}}{\delta \mathbf{h}^{(\tau-1)}} + \nabla_{\mathbf{h}^{(\tau)}}C \frac{\delta \mathbf{h}^{(\tau)}}{\delta \mathbf{h}^{(\tau-1)}} \\
    \nabla_{\mathbf{h}^{(\tau-2)}} C &= \color{red}\nabla_{\mathbf{o}^{(\tau-2)}}C \frac{\delta \mathbf{o}^{(\tau-2)}}{\delta \mathbf{h}^{(\tau-2)}} + \color{blue}\nabla_{\mathbf{h}^{(\tau-1)}}C \color{red} \frac{\delta \mathbf{h}^{(\tau-1)}}{\delta \mathbf{h}^{(\tau-2)}} \\
    \nabla_{\mathbf{h}^{(\tau-3)}} C &= \nabla_{\mathbf{o}^{(\tau-3)}}C \frac{\delta \mathbf{o}^{(\tau-3)}}{\delta \mathbf{h}^{(\tau-3)}} + \color{red}\nabla_{\mathbf{h}^{(\tau-2)}}C \color{black} \frac{\delta \mathbf{h}^{(\tau-2)}}{\delta \mathbf{h}^{(\tau-3)}} \\
    &= \nabla_{\mathbf{o}^{(\tau-3)}}C \frac{\delta \mathbf{o}^{(\tau-3)}}{\delta \mathbf{h}^{(\tau-3)}} + \color{red}\left(\nabla_{\mathbf{o}^{(\tau-2)}}C \frac{\delta \mathbf{o}^{(\tau-2)}}{\delta \mathbf{h}^{(\tau-2)}} + \color{blue}\nabla_{\mathbf{h}^{(\tau-1)}}C\color{red} \frac{\delta \mathbf{h}^{(\tau-1)}}{\delta \mathbf{h}^{(\tau-2)}} \right) \color{black} \frac{\delta \mathbf{h}^{(\tau-2)}}{\delta \mathbf{h}^{(\tau-3)}} \\
    &= \nabla_{\mathbf{o}^{(\tau-3)}}C \frac{\delta \mathbf{o}^{(\tau-3)}}{\delta \mathbf{h}^{(\tau-3)}} + \color{red}\left(\nabla_{\mathbf{o}^{(\tau-2)}}C \frac{\delta \mathbf{o}^{(\tau-2)}}{\delta \mathbf{h}^{(\tau-2)}} + \color{blue}\left(\color{blue}\nabla_{\mathbf{o}^{(\tau-1)}}C \frac{\delta \mathbf{o}^{(\tau-1)}}{\delta \mathbf{h}^{(\tau-1)}} + \nabla_{\mathbf{h}^{(\tau)}}C \frac{\delta \mathbf{h}^{(\tau)}}{\delta \mathbf{h}^{(\tau-1)}} \right)\color{red} \frac{\delta \mathbf{h}^{(\tau-1)}}{\delta \mathbf{h}^{(\tau-2)}} \right) \color{black} \frac{\delta \mathbf{h}^{(\tau-2)}}{\delta \mathbf{h}^{(\tau-3)}} \\
    \shortintertext{Generally:}
    \nabla_{\mathbf{h}^{(t)}} C &= \nabla_{\mathbf{o}^{(t)}}C \frac{\delta \mathbf{o}^{(t)}}{\delta \mathbf{h}^{(t)}} + \nabla_{\mathbf{h}^{(t+1)}}C \frac{\delta \mathbf{h}^{(t+1)}}{\delta \mathbf{h}^{(t)}}
    % &= \nabla_{\mathbf{o}^{(\tau-2)}}C \frac{\delta \mathbf{o}^{(\tau-2)}}{\delta \mathbf{h}^{(\tau-2)}} + \color{blue}\left(\nabla_{\mathbf{o}^{(\tau-1)}}C \frac{\delta \mathbf{o}^{(\tau-1)}}{\delta \mathbf{h}^{(\tau-1)}} + \nabla_{\mathbf{h}^{(\tau)}}C \frac{\delta \mathbf{h}^{(\tau)}}{\delta \mathbf{h}^{(\tau-1)}}\right)\color{black}\frac{\delta \mathbf{h}^{(\tau-1)}}{\delta \mathbf{h}^{(\tau-2)}}
    % \nabla_{\mathbf{h}^{(0)}} C &= \nabla_{\mathbf{o}^{(0)}}C + \nabla_{\mathbf{h}^{(\tau-3)}}C \\
\end{align}\par
Some parts of the equations above have been colored to emphasize the parts that we take with us from one gradient calculation to the next. \par
It is observable that the gradient at time $t$ is indeed dependent on all later time steps $t + 1, t + 2, t + n$, as well as the current output. The influence from current output can be seen before the first (+), while the influence from previous states can be observed after the first (+). \par

The final step is to calculate the gradient of the parameter nodes $\mathbf{U, W, b, V, c}$. To find these gradients, we must differentiate $\mathbf{h}^{(t)}$. Calculating the derivative of $\mathbf{h}^{(t)}$ involves differentiating the activation function using the chain rule. In the equations below, the chain rule is applied to differentiate the activations with respect to different parameters, but the activation function itself is not differentiated because it depends on which activation function in use. It is instead denoted as $\nabla_{activation}$.
\begin{align*}
    \nabla_\mathbf{c}C &= \sum_t \left(\nabla_{\mathbf{o}^{(t)}}C\right) \frac{\delta \mathbf{o}^{(t)}}{\delta \mathbf{c}^{(t)}} &&= \sum_t \left(\nabla_{\mathbf{o}^{(t)}}C\right) \cdot 1 \\
    \nabla_\mathbf{V}C &= \sum_t \left(\nabla_{\mathbf{o}^{(t)}}C\right) \frac{\delta \mathbf{o}^{(t)}}{\delta \mathbf{h}^{(t)}} &&= \sum_t \left(\nabla_{\mathbf{o}^{(t)}}C\right)\mathbf{h^{(t)}} \\
    \nabla_\mathbf{b}C &= \sum_t \left(\nabla_{\mathbf{h}^{(t)}}C\right) \frac{\delta \mathbf{h}^{(t)}}{\delta \mathbf{b}^{(t)}} &&= \sum_t \left(\nabla_{\mathbf{h}^{(t)}}C\right) \left(\nabla_{activation}\right) \cdot 1 \\
    \nabla_\mathbf{W}C &= \sum_t \left(\nabla_{\mathbf{h}^{(t)}}C\right) \frac{\delta \mathbf{h}^{(t)}}{\delta \mathbf{W}^{(t)}} &&= \sum_t \left(\nabla_{\mathbf{h}^{(t)}}C\right) \left(\nabla_{activation}\right) \cdot \mathbf{h}^{(t-1)} \\
    \nabla_\mathbf{U}C &= \sum_t \left(\nabla_{\mathbf{h}^{(t)}}C\right) \frac{\delta \mathbf{h}^{(t)}}{\delta \mathbf{U}^{(t)}} &&= \sum_t \left(\nabla_{\mathbf{h}^{(t)}}C\right) \left(\nabla_{activation}\right) \cdot \mathbf{x}^{(t-1)}\\
\end{align*}


The code written for BPTT is tricky to read. Below is conversions from math to code for gradient calculation of eq. (21):
\begin{align*}
    \nabla_{\mathbf{o}^{(t)}} C &= \hat{y}^{(t)} - \mathbf{1}_{i=y^{(t)}} &&\Rightarrow \texttt{w\_hy} \\
    \frac{\delta \mathbf{o}^{(t)}}{\delta \mathbf{h}^{(t)}} &= \mathbf{V} &&\Rightarrow \texttt{grad\_o\_Cost} \\
    \nabla_{\mathbf{h}^{(t+1)}} C &= \nabla_{\mathbf{o}^{(t+1)}}C \frac{\delta \mathbf{o}^{(t+1)}}{\delta \mathbf{h}^{(t+1)}} + \nabla_{\mathbf{h}^{(\tau)}}C \frac{\delta \mathbf{h}^{(t+2)}}{\delta \mathbf{h}^{(t+1)}} &&\Rightarrow \texttt{prev\_grad\_h\_Cost} \\
    \frac{\delta \mathbf{h}^{(t+1)}}{\delta \mathbf{h}^{(t)}} &= \mathbf{W} &&\Rightarrow \texttt{w\_hh} \\
    \nabla_{\mathbf{h}^{(t)}} C &&&\Rightarrow \texttt{grad\_h\_Cost} \\
\end{align*}

\begin{align*}
    z_t &= Ux_t + Wh_{t-1} + b_t \\
    h_t &=  \sigma(z_t) \\
    o_t &=  Vh_t + c_t \\
    y_t &=  \sigma(Vh_t + c_t) \\
    C^t &= C(y_t,\hat{y_t}) \\
    \frac{\delta C_t}{\delta W} &= \frac{\delta C_t}{\delta y_t}
    \cdot \sum^n_{k=1}\left[\frac{\delta h_t}{\delta h_.}\cdot\right] \frac{\delta h_k}{W}
\end{align*}

\end{document}