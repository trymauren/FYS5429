
@book{goodfellow_deep_2016,
	address = {Cambridge, Mass},
	series = {Adaptive computation and machine learning},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	language = {eng},
	publisher = {The MIT press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@incollection{james_deep_2023,
	address = {Cham},
	title = {Deep {Learning}},
	isbn = {978-3-031-38747-0},
	url = {https://doi.org/10.1007/978-3-031-38747-0_10},
	language = {en},
	urldate = {2024-03-29},
	booktitle = {An {Introduction} to {Statistical} {Learning}: with {Applications} in {Python}},
	publisher = {Springer International Publishing},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert and Taylor, Jonathan},
	editor = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert and Taylor, Jonathan},
	year = {2023},
	doi = {10.1007/978-3-031-38747-0_10},
	pages = {399--467},
}

@misc{nabi_recurrent_2019,
	title = {Recurrent {Neural} {Networks} ({RNNs})},
	url = {https://towardsdatascience.com/recurrent-neural-networks-rnns-3f06d7653a85},
	abstract = {Implementing an RNN from scratch in Python.},
	language = {en},
	urldate = {2024-03-29},
	journal = {Medium},
	author = {Nabi, Javaid},
	month = jul,
	year = {2019},
}

@misc{ines_montani_explosionspacy_2023,
	title = {explosion/{spaCy}: v3.7.2: {Fixes} for {APIs} and requirements},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {explosion/{spaCy}},
	url = {https://zenodo.org/doi/10.5281/zenodo.1212303},
	abstract = {✨ New features and improvements



Update \_\_all\_\_ fields (\#13063).


🔴 Bug fixes



\#13035: Remove Pathy requirement.

\#13053: Restore spacy.cli.project API.

\#13057: Support Any comparisons for Token and Span.


📖 Documentation and examples



Many updates for spacy-llm including Azure OpenAI, PaLM, and Mistral support.

Various documentation corrections.


👥 Contributors

@adrianeboyd, @honnibal, @ines, @rmitsch, @svlandeg},
	urldate = {2024-03-29},
	publisher = {[object Object]},
	author = {Ines Montani and Matthew Honnibal and Matthew Honnibal and Adriane Boyd and Sofie Van Landeghem and Henning Peters},
	month = oct,
	year = {2023},
	doi = {10.5281/ZENODO.1212303},
}

@misc{noauthor_notitle_nodate,
}

@article{jatnika_word2vec_2019,
	series = {The 4th {International} {Conference} on {Computer} {Science} and {Computational} {Intelligence} ({ICCSCI} 2019) : {Enabling} {Collaboration} to {Escalate} {Impact} of {Research} {Results} for {Society}},
	title = {{Word2Vec} {Model} {Analysis} for {Semantic} {Similarities} in {English} {Words}},
	volume = {157},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050919310713},
	doi = {10.1016/j.procs.2019.08.153},
	abstract = {This paper examines the calculation of the similarity between words in English using word representation techniques. Word2Vec is a model used in this paper to represent words into vector form. The model in this study was formed using the 320,000 articles in the English Wikipedia as the corpus and then Cosine Similarity calculation method is used to determine the similarity value. This model then tested by the test set gold standard WordSim-353 as many as 353 pairs of words and SimLex-999 as many as 999 pairs of words, which have been labelled with similarity values according to human judgment. Pearson Correlation was used to find out the accuracy of the correlation. The results of the correlation from this study are 0.665 for WordSim-353 and 0.284 for SimLex-999 using the Windows size 9 and 300 vector dimension configurations.},
	urldate = {2024-03-29},
	journal = {Procedia Computer Science},
	author = {Jatnika, Derry and Bijaksana, Moch Arif and Suryani, Arie Ardiyanti},
	month = jan,
	year = {2019},
	keywords = {Cosine Similarity, Pearson Correlation, Word2Vec},
	pages = {160--167},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {1986 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2024-03-29},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	note = {Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, Science, multidisciplinary},
	pages = {533--536},
}

@misc{wang_benchmarking_2019,
	title = {Benchmarking {TPU}, {GPU}, and {CPU} {Platforms} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/1907.10701},
	abstract = {Training deep learning models is compute-intensive and there is an industry-wide trend towards hardware specialization to improve performance. To systematically benchmark deep learning platforms, we introduce ParaDnn, a parameterized benchmark suite for deep learning that generates end-to-end models for fully connected (FC), convolutional (CNN), and recurrent (RNN) neural networks. Along with six real-world models, we benchmark Google's Cloud TPU v2/v3, NVIDIA's V100 GPU, and an Intel Skylake CPU platform. We take a deep dive into TPU architecture, reveal its bottlenecks, and highlight valuable lessons learned for future specialized system design. We also provide a thorough comparison of the platforms and find that each has unique strengths for some types of models. Finally, we quantify the rapid performance improvements that specialized software stacks provide for the TPU and GPU platforms.},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Wang, Yu Emma and Wei, Gu-Yeon and Brooks, David},
	month = oct,
	year = {2019},
	note = {arXiv:1907.10701 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Performance, Statistics - Machine Learning},
}

@article{church_word2vec_2017,
	title = {{Word2Vec}},
	volume = {23},
	issn = {1351-3249, 1469-8110},
	url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/word2vec/B84AE4446BD47F48847B4904F0B36E0B#},
	doi = {10.1017/S1351324916000334},
	abstract = {My last column ended with some comments about Kuhn and word2vec. Word2vec has racked up plenty of citations because it satisifies both of Kuhn’s conditions for emerging trends: (1) a few initial (promising, if not convincing) successes that motivate early adopters (students) to do more, as well as (2) leaving plenty of room for early adopters to contribute and benefit by doing so. The fact that Google has so much to say on ‘How does word2vec work’ makes it clear that the definitive answer to that question has yet to be written. It also helps citation counts to distribute code and data to make it that much easier for the next generation to take advantage of the opportunities (and cite your work in the process).},
	language = {en},
	number = {1},
	urldate = {2024-03-29},
	journal = {Natural Language Engineering},
	author = {Church, Kenneth Ward},
	month = jan,
	year = {2017},
	pages = {155--162},
}

@article{harrisArrayProgrammingNumPy2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	copyright = {2020 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2649-2},
	doi = {10.1038/s41586-020-2649-2},
	abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
	language = {en},
	number = {7825},
	urldate = {2024-03-29},
	journal = {Nature},
	author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, Stéfan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and del Río, Jaime Fernández and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	month = sep,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Computational science, Computer science, Software, Solar physics},
	pages = {357--362},
}

@misc{tannerAdagrad,
	title = {Adagrad},
	url = {https://ml-explained.com/blog/adagrad-explained},
	abstract = {Articles focused on Machine Learning, Artificial Intelligence and Data Science},
	language = {en},
	urldate = {2024-03-21},
	journal = {Machine Learning Explained},
	author = {Tanner, Gilbert},
}

@misc{kingmaAdamMethodStochastic2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{duchiAdaptiveSubgradientMethods,
	title = {Adaptive {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization}},
	abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to ﬁnd needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which signiﬁcantly simpliﬁes setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efﬁcient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	language = {en},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
}

@misc{schmidtRecurrentNeuralNetworks2019,
	title = {Recurrent {Neural} {Networks} ({RNNs}): {A} gentle {Introduction} and {Overview}},
	shorttitle = {Recurrent {Neural} {Networks} ({RNNs})},
	url = {http://arxiv.org/abs/1912.05911},
	doi = {10.48550/arXiv.1912.05911},
	abstract = {State-of-the-art solutions in the areas of "Language Modelling \& Generating Text", "Speech Recognition", "Generating Image Descriptions" or "Video Tagging" have been using Recurrent Neural Networks as the foundation for their approaches. Understanding the underlying concepts is therefore of tremendous importance if we want to keep up with recent or upcoming publications in those areas. In this work we give a short overview over some of the most important concepts in the realm of Recurrent Neural Networks which enables readers to easily understand the fundamentals such as but not limited to "Backpropagation through Time" or "Long Short-Term Memory Units" as well as some of the more recent advances like the "Attention Mechanism" or "Pointer Networks". We also give recommendations for further reading regarding more complex topics where it is necessary.},
	urldate = {2024-03-16},
	publisher = {arXiv},
	author = {Schmidt, Robin M.},
	month = nov,
	year = {2019},
	note = {arXiv:1912.05911 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{SoftmaxFunctionIts,
	title = {The {Softmax} function and its derivative - {Eli} {Bendersky}'s website},
	url = {https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/},
	urldate = {2024-02-15},
}

@article{sherstinskyFundamentalsRecurrentNeural2020,
	title = {Fundamentals of {Recurrent} {Neural} {Network} ({RNN}) and {Long} {Short}-{Term} {Memory} ({LSTM}) network},
	volume = {404},
	issn = {0167-2789},
	url = {https://www.sciencedirect.com/science/article/pii/S0167278919305974},
	doi = {10.1016/j.physd.2019.132306},
	abstract = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of “unrolling” an RNN is routinely presented without justification throughout the literature. The goal of this tutorial is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in Signal Processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the “Vanilla LSTM”1 1The nickname “Vanilla LSTM” symbolizes this model’s flexibility and generality (Greff et al., 2015). network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this treatise valuable as well.},
	urldate = {2024-02-13},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Sherstinsky, Alex},
	month = mar,
	year = {2020},
	keywords = {Convolutional input context windows, External input gate, LSTM, RNN, RNN unfolding/unrolling},
	pages = {132306},
}

@misc{262588213843476MinimalCharacterlevelLanguage,
	title = {Minimal character-level language model with a {Vanilla} {Recurrent} {Neural} {Network}, in {Python}/numpy},
	url = {https://gist.github.com/karpathy/d4dee566867f8291f086},
	abstract = {Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy - min-char-rnn.py},
	language = {en},
	urldate = {2024-02-06},
	journal = {Gist},
	author = {262588213843476},
}

@misc{BackPropagationTime2020,
	title = {Back {Propagation} through time - {RNN}},
	url = {https://www.geeksforgeeks.org/ml-back-propagation-through-time/},
	abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
	language = {en-US},
	urldate = {2024-02-04},
	journal = {GeeksforGeeks},
	month = apr,
	year = {2020},
	note = {Section: Machine Learning},
}

@misc{RNNImplementationusingNumPyRNNImplementation,
	title = {{RNN}-{Implementation}-using-{NumPy}/{RNN} {Implementation} using {NumPy}.ipynb at master · {JY}-{Yoon}/{RNN}-{Implementation}-using-{NumPy}},
	url = {https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb},
	abstract = {RNN-Implementation-using-NumPy. Contribute to JY-Yoon/RNN-Implementation-using-NumPy development by creating an account on GitHub.},
	language = {en},
	urldate = {2024-01-26},
	journal = {GitHub},
}

@misc{DdbourginNumpymlMachine,
	title = {ddbourgin/numpy-ml: {Machine} learning, in numpy},
	url = {https://github.com/ddbourgin/numpy-ml},
	urldate = {2024-01-26},
}
